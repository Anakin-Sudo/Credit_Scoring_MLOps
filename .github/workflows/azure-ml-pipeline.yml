name: AzureML Pipeline

on:
  workflow_dispatch:
    inputs:
      data_blob:
        description: 'Name of the blob containing raw credit data'
        required: true
        default: 'german_credit.csv'

jobs:
  submit-pipeline:
    runs-on: ubuntu-latest
    env:
      SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      RESOURCE_GROUP: ${{ secrets.AZURE_RESOURCE_GROUP }}
      WORKSPACE_NAME: ${{ secrets.AZURE_WORKSPACE_NAME }}
      COMPUTE_NAME: ${{ secrets.AZURE_COMPUTE_NAME }}
      # Name of the CSV blob to ingest.  This is supplied via the
      # `data_blob` input when triggering the workflow.  If you want to run
      # the pipeline automatically (e.g. on schedule) you can hardâ€‘code
      # the value here or store it as a secret instead.
      BLOB_NAME: ${{ inputs.data_blob }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Azure Login
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r Credit_Scoring/requirements.txt
          pip install azure-identity azure-ai-ml python-dotenv azure-storage-blob

      - name: Submit Azure ML pipeline
        run: |
          # Write required env variables to .env file for submit_pipeline.py
          echo "SUBSCRIPTION_ID=${SUBSCRIPTION_ID}" >> Credit_Scoring/.env
          echo "RESOURCE_GROUP=${RESOURCE_GROUP}" >> Credit_Scoring/.env
          echo "WORKSPACE_NAME=${WORKSPACE_NAME}" >> Credit_Scoring/.env
          echo "COMPUTE_NAME=${COMPUTE_NAME}" >> Credit_Scoring/.env
          echo "BLOB_NAME=${BLOB_NAME}" >> Credit_Scoring/.env
          python Credit_Scoring/pipelines/submit_pipeline.py